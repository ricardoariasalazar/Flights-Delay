{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flights Delay Big Data Analysis\n",
    "\n",
    "## Table of contents<a class=\"anchor\" id=\"table\"></a>\n",
    "\n",
    "* [1 Working with RDD](#1)\n",
    "* [1.1 Data Preparation and Loading](#1.1)\n",
    "* [1.1.1 Creating SparkSession & SparkContext](#OneOneOne)\n",
    "* [1.1.2 Read CSV files, Preprocessing, and final(formatted data) RDD for each file](#OneOneTwo)\n",
    "* [1.1.2.1 Flights RDD](#1.1.2.1)\n",
    "* [1.1.2.2 Airports RDD](#1.1.2.2)\n",
    "* [1.1.3 Show RDD number of columns, and number of records](#1.1.3)\n",
    "* [1.2 Dataset flights partitioning](#1.2)\n",
    "* [1.2.1 Obtain the maximum arrival delay ](#1.2.1)\n",
    "* [1.2.2 Obtain the minimum arrival delay ](#1.2.2)\n",
    "* [1.2.3 Define hash partitioning](#1.2.3)\n",
    "* [1.2.4 Display the records in each partition](#1.2.4)\n",
    "* [1.3 Query RDD](#1.3)\n",
    "* [1.3.1 Collect a total number of flights for each month for all flights](#1.3.1)\n",
    "* [1.3.2 Collect the average delay for each month for all flights](#1.3.2)\n",
    "* [2 Working with DataFrames](#2)\n",
    "* [2.1 Data Preparation and Loading](#2.1)\n",
    "* [2.1.1 Define DataFrames](#2.1.1)\n",
    "* [2.1.2 Display the Scheme of DataFrames](#2.1.2)\n",
    "* [2.1.3 Transform date-time and location column](#2.1.3)\n",
    "* [2.2.1 January Flights Events with ANC airport](#2.2.1)\n",
    "* [2.2.2 Average Arrival Delay From Origin to Destination](#2.2.2)\n",
    "* [2.2.3 Join Query with Airports DataFrame](#2.2.3)\n",
    "* [2.3 Analysis](#2.3.1)\n",
    "* [2.3.1 Relationship between day of week with mean arrival delay, total time delay, and count flights](#2.3.1)\n",
    "* [2.3.2 Display mean arrival delay each month](#2.3.2)\n",
    "* [2.3.3 Relationship between mean departure delay and mean arrival delay](#2.3.3)\n",
    "* [3 RDDs vs DataFrame vs Spark SQL](#3)\n",
    "* [3.1 RDD Operation](#3.1)\n",
    "* [3.2 DataFrame Operation](#3.1)\n",
    "* [3.3 Spark SQL Operation](#3.1)\n",
    "* [3.4 Discussion](#3.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1 Working with RDD<a class=\"anchor\" id=\"1\"></a>\n",
    "## 1.1 Data Preparation and Loading<a class=\"anchor\" id=\"1.1\"></a>\n",
    "### 1.1.1 Create SparkSession and SparkContext<a class=\"anchor\" id=\"OneOneOne\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkConf class into program\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# local[*]: run Spark in local mode with as many working processors as logical cores on your machine\n",
    "# If we want Spark to run locally with 'k' worker threads, we can specify as \"local[k]\".\n",
    "master = \"local[*]\"\n",
    "# The `appName` field is a name to be shown on the Spark cluster UI page\n",
    "app_name = \"Assignment 1 - Ricardo Arias (ID: 30550971)\"\n",
    "# Setup configuration parameters for Spark\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name)\n",
    "\n",
    "# Import SparkContext and SparkSession classes\n",
    "from pyspark import SparkContext # Spark\n",
    "from pyspark.sql import SparkSession # Spark SQL\n",
    "\n",
    "# Method 1: Using SparkSession\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Import CSV files and Make RDD for each file<a class=\"anchor\" id=\"OneOneTwo\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two different ways to do this, the first way could be importing the files as `CSV files` and then convert them into RDD with the function `sc.parallelize()`, and the second way that is the one we are going to use in this case is using the function `sc.textFile()` to import the CSV file as a RDD in just one step.\n",
    "\n",
    "For this assignment we will create a function which loads the csv data into the RDD object using the function `sc.textFile()`, then we will split the data using commas (,) using the function `split()`, after we will remove the header from the RDD using `first()` and `filter`, then we will remove the **NULL** values in the dataset, changing them by 0, so it is easier to handle as number or string. Finally, we parse the RDD object into the desired format (integer, float, or string) using lists given by the user as the columns they want to convert to each fromat. All these steps are done by the function `csv_to_RDD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "def csv_to_RDD(path, int_cols=[], float_cols=[]): # Path of the file, list with the name of the columns that are int and float\n",
    "    rdd = sc.textFile(path) # Read the CSV file and convert it into a RDD\n",
    "    rdd = rdd.map(lambda line: line.split(',')) # Split data using comma (,)\n",
    "    header_rdd = rdd.first() # Set the header as the first row\n",
    "    rdd = rdd.filter(lambda row: row != header_rdd)   # filter out header\n",
    "    rdd = rdd.map(lambda x: [0 if len(i) == 0 else i for i in x]) # Change null values with 0\n",
    "    \n",
    "    if (int_cols and float_cols):\n",
    "        # Use map and list comprenhension to set columns as integer, float or string depending of both lists above\n",
    "        rdd = rdd.map(lambda x: [int(x[i]) if header_rdd[i] in int_cols \\\n",
    "                                 else float(x[i]) if header_rdd[i] in float_cols else str(x[i]) for i in range(len(x))])\n",
    "    \n",
    "    # Convert the values from list to sql Row \n",
    "    rdd = rdd.map(lambda x: Row(**dict(zip(header_rdd, x))))\n",
    "    \n",
    "    return(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the function is created, we proceed to import all the CSV dataset given"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2.1 Flights RDD <a class=\"anchor\" id=\"1.1.2.1\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the function to import the data, we proceed to declare the columns that will be integers and floats before we  use the function to import the `flight` datasets as one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare columns that are integer and float\n",
    "int_cols = ['YEAR', 'MONTH', 'DAY','DAY_OF_WEEK', 'FLIGHT_NUMBER']\n",
    "float_cols = ['DEPARTURE_DELAY', 'ARRIVAL_DELAY', 'ELAPSED_TIME', 'AIR_TIME', 'DISTANCE', 'TAXI_IN', 'TAXI_OUT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import the data using the function, and check that the information was parsed correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(YEAR=2015, MONTH=6, DAY=26, DAY_OF_WEEK=5, AIRLINE='EV', FLIGHT_NUMBER=4951, TAIL_NUMBER='N707EV', ORIGIN_AIRPORT='BHM', DESTINATION_AIRPORT='LGA', SCHEDULED_DEPARTURE='630', DEPARTURE_TIME='629', DEPARTURE_DELAY=-1.0, TAXI_OUT=13.0, WHEELS_OFF='642', SCHEDULED_TIME='155', ELAPSED_TIME=141.0, AIR_TIME=113.0, DISTANCE=866.0, WHEELS_ON='935', TAXI_IN=15.0, SCHEDULED_ARRIVAL='1005', ARRIVAL_TIME='950', ARRIVAL_DELAY=-15.0, DIVERTED='0', CANCELLED='0', CANCELLATION_REASON='0', AIR_SYSTEM_DELAY='0', SECURITY_DELAY='0', AIRLINE_DELAY='0', LATE_AIRCRAFT_DELAY='0', WEATHER_DELAY='0')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights = csv_to_RDD('./flight-delays/flight*.csv', int_cols, float_cols)\n",
    "flights.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2.2 Airports RDD <a class=\"anchor\" id=\"1.1.2.2\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same with the Airport dataset, but in this case we leave all the columns as `str`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(IATA_CODE='ABE', AIRPORT='Lehigh Valley International Airport', CITY='Allentown', STATE='PA', COUNTRY='USA', LATITUDE='40.65236', LONGITUDE='-75.44040')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airports = csv_to_RDD('./flight-delays/airports.csv')\n",
    "airports.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Show RDD number of columns, and number of records <a class=\"anchor\" id=\"1.1.3\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing the RDD, we will display the number of columns, the total number of records, and display the number of partitions of each RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows of Flights RDD: 582184 \n",
      "Number of Columns of Flights RDD: 31 \n",
      "Number of Partitions of Flight RDD: 20\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Rows of Flights RDD:\", flights.count(),'\\nNumber of Columns of Flights RDD:', len(flights.take(1)[0]),\\\n",
    "      \"\\nNumber of Partitions of Flight RDD:\",flights.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows of Airports RDD: 322 \n",
      "Number of Columns of Airports RDD: 7 \n",
      "Number of Partitions of Airports RDD: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Rows of Airports RDD:\", airports.count(),'\\nNumber of Columns of Airports RDD:', len(airports.take(1)[0]),\\\n",
    "      \"\\nNumber of Partitions of Airports RDD:\",airports.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Dataset Partitioning <a class=\"anchor\" id=\"1.2\"></a>\n",
    "\n",
    "By looking at the results above we find out that the flights RDD, has 20 different partitions by default, which matches with the number of documents imported. In this particular case, by default Pyspark assume each document as one partition. \n",
    "On the other hand, the airports RDD only have 2 partitions, even though we imported 1 document. This is because the default number of partitions set by Pyspark is 2 when importing a single document. That way we can observe the two default configurations that Pyspark has.\n",
    "\n",
    "With the RDDs already transformed we can proceed to do the parallel search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Obtain the maximum arrival delay <a class=\"anchor\" id=\"1.2.1\"></a>\n",
    "[Back to top](#table)\n",
    "\n",
    "Using the function of rdd `max` we can find the maximum value of a specific column. In this case we are finding the maximum ARRIVAL_DELAY in the whole dataset. That in this case is the flight 11 of the airine AS, that on 13 of July of 2015 had a delay of 1665.0 *minutes* (the units were assumed as the metadata does not mention it). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(YEAR=2015, MONTH=9, DAY=13, DAY_OF_WEEK=7, AIRLINE='AA', FLIGHT_NUMBER=1063, TAIL_NUMBER='N3CAAA', ORIGIN_AIRPORT='SAN', DESTINATION_AIRPORT='DFW', SCHEDULED_DEPARTURE='700', DEPARTURE_TIME='1050', DEPARTURE_DELAY=1670.0, TAXI_OUT=26.0, WHEELS_OFF='1116', SCHEDULED_TIME='179', ELAPSED_TIME=174.0, AIR_TIME=142.0, DISTANCE=1171.0, WHEELS_ON='1538', TAXI_IN=6.0, SCHEDULED_ARRIVAL='1159', ARRIVAL_TIME='1544', ARRIVAL_DELAY=1665.0, DIVERTED='0', CANCELLED='0', CANCELLATION_REASON='0', AIR_SYSTEM_DELAY='0', SECURITY_DELAY='0', AIRLINE_DELAY='1665', LATE_AIRCRAFT_DELAY='0', WEATHER_DELAY='0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.max(key=lambda x: x.ARRIVAL_DELAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Obtain the minimum arrival time <a class=\"anchor\" id=\"1.2.2\"></a>\n",
    "[Back to top](#table)\n",
    "\n",
    "Using the function of rdd `min` we can find the minimum value of a specific column. In this case we are finding the minimum ARRIVAL_DELAY in the whole dataset. That in this case is the same flight 11 of the airine AS, that on 21 of January of 2015 had a delay of -82.0 *minutes* (the units were assumed as the metadata does not mention it). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(YEAR=2015, MONTH=1, DAY=21, DAY_OF_WEEK=3, AIRLINE='AS', FLIGHT_NUMBER=11, TAIL_NUMBER='N467AS', ORIGIN_AIRPORT='EWR', DESTINATION_AIRPORT='SEA', SCHEDULED_DEPARTURE='1720', DEPARTURE_TIME='1705', DEPARTURE_DELAY=-15.0, TAXI_OUT=13.0, WHEELS_OFF='1718', SCHEDULED_TIME='389', ELAPSED_TIME=322.0, AIR_TIME=305.0, DISTANCE=2402.0, WHEELS_ON='1923', TAXI_IN=4.0, SCHEDULED_ARRIVAL='2049', ARRIVAL_TIME='1927', ARRIVAL_DELAY=-82.0, DIVERTED='0', CANCELLED='0', CANCELLATION_REASON='0', AIR_SYSTEM_DELAY='0', SECURITY_DELAY='0', AIRLINE_DELAY='0', LATE_AIRCRAFT_DELAY='0', WEATHER_DELAY='0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.min(key=lambda x: x.ARRIVAL_DELAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(YEAR=2015, MONTH=7, DAY=7, DAY_OF_WEEK=2, AIRLINE='OO', FLIGHT_NUMBER=7438, TAIL_NUMBER='N454SW', ORIGIN_AIRPORT='SLC', DESTINATION_AIRPORT='EKO', SCHEDULED_DEPARTURE='1345', DEPARTURE_TIME='1343', DEPARTURE_DELAY=-2.0, TAXI_OUT=20.0, WHEELS_OFF='1403', SCHEDULED_TIME='52', ELAPSED_TIME=58.0, AIR_TIME=33.0, DISTANCE=200.0, WHEELS_ON='1336', TAXI_IN=5.0, SCHEDULED_ARRIVAL='1337', ARRIVAL_TIME='1341', ARRIVAL_DELAY=4.0, DIVERTED='0', CANCELLED='0', CANCELLATION_REASON='0', AIR_SYSTEM_DELAY='0', SECURITY_DELAY='0', AIRLINE_DELAY='0', LATE_AIRCRAFT_DELAY='0', WEATHER_DELAY='0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.max(key=lambda x: x.FLIGHT_NUMBER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Define hash partitioning function <a class=\"anchor\" id=\"1.2.3\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define a hash function, this one will be useful to partition the data. In this case we are going to sum up all the digits of the key, and if the key is not numeric we will sum up the digits of the lenght of the string. With this hash key we guarantee an endless amount of possibilities to partition the data in as many partitions as the user wants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hash Function to implement Hash Partitioning \n",
    "\n",
    "def hash_function(key):\n",
    "    if type(key) == str: # If the key is string\n",
    "        key = len(key) # Convert the key into the lenght of the key\n",
    "        \n",
    "    total = 0\n",
    "    for digit in str(key): # Get each digit of the key\n",
    "        total += int(digit) # Sum up all the digits of the key\n",
    "    return total # Return the sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 Display the records in each partition <a class=\"anchor\" id=\"1.2.4\"></a>\n",
    "[Back to top](#table)\n",
    "\n",
    "Then we will create the function that will allow us to observe the number of partitions and the number of records per partition in a specific RDD inputted by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.rdd import RDD\n",
    "\n",
    "#A Function to print the data items in each RDD\n",
    "def print_partitions(data):\n",
    "    if isinstance(data, RDD):  # If the data is a RDD\n",
    "        numPartitions = data.getNumPartitions() # Get the number of partitions of the RDD\n",
    "        partitions = data.glom().collect() # Collect the records of each partition\n",
    "    else: # If the data is not a RDD\n",
    "        numPartitions = data.rdd.getNumPartitions() # Convert the data into RDD and get the number of partitions\n",
    "        partitions = data.rdd.glom().collect() # Convert the data into RDD and collect the records of each partition\n",
    "    \n",
    "    print(f\"####### NUMBER OF PARTITIONS: {numPartitions}\")\n",
    "    for index, partition in enumerate(partitions): # For each partition \n",
    "        # show partition if it is not empty\n",
    "        if len(partition) > 0:\n",
    "            print(f\"Partition {index}: {len(partition)} records\") # Print partition number and number of records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we created these two functions we proceed to partition each RDD and then visualize the information of each RDD after this procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flights\n",
    "The key of a RDD is the first column, in this case, the key column is year. By exploring the data we found out that all the values of this column are `2015`. Therefore, if we use this column as a key for partitioning the data, all the records would end up in the same partition, and the others will remain empty. To avoid this problem we will change the key to be `FLIGHT_NUMBER` and that way we can use the hash function correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### NUMBER OF PARTITIONS: 4\n",
      "Partition 0: 145384 records\n",
      "Partition 1: 147688 records\n",
      "Partition 2: 145302 records\n",
      "Partition 3: 143810 records\n"
     ]
    }
   ],
   "source": [
    "# hash partitioning of flights RDD\n",
    "flight_hash = flights.keyBy(lambda x: x.FLIGHT_NUMBER).partitionBy(4, hash_function)\n",
    "print_partitions(flight_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airports\n",
    "As we did with flights RDD we explored the values of the key of airports RDD and in this case is the IATA code. It is a string so the hash function will convert it to the lenght of it, but again most of the IATA codes have only 3 digits. Therefore, if we use this column as a key for partitioning the data we will find the same error, all the records would end up in the same partition, and the others will remain empty. To avoid this problem we will change the key to be `AIRPORT` and that way we can use the hash function correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### NUMBER OF PARTITIONS: 4\n",
      "Partition 0: 79 records\n",
      "Partition 1: 85 records\n",
      "Partition 2: 73 records\n",
      "Partition 3: 85 records\n"
     ]
    }
   ],
   "source": [
    "# hash partitioning of airports RDD\n",
    "airports_hash = airports.keyBy(lambda x: x.AIRPORT).partitionBy(4, hash_function)\n",
    "print_partitions(airports_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When displaying the number of partitions of each RDD using the hash function, we see that the data is pretty balanced in both cases. Each partition has around the 25% of the data, what is equivalent to the proportion of data if it was evenlly distributed. This means that the hash function is working properly, because is preventing to overload on processor and underload some others, guarantying an optimal partitioning of the data and a correct processing of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Query RDD  <a class=\"anchor\" id=\"1.3\"></a>\n",
    "### 1.3.1 Collect a total number of flights for each month <a class=\"anchor\" id=\"1.3.1\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using RDD we can not retrieve the name of the column, but in this case the key (first columns) is the number of the month. The second one is the number of flights of each month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 47136),\n",
       " (2, 42798),\n",
       " (3, 50816),\n",
       " (4, 48810),\n",
       " (5, 49691),\n",
       " (6, 50256),\n",
       " (7, 52065),\n",
       " (8, 50524),\n",
       " (9, 46733),\n",
       " (10, 48680),\n",
       " (11, 46809),\n",
       " (12, 47866)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.map(lambda x: (x.MONTH, x.FLIGHT_NUMBER)).groupByKey().mapValues(list).mapValues(lambda x: len(x)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Collect the average delay for each month <a class=\"anchor\" id=\"1.3.2\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the same code we used above, in this case we are not going to count how many flights  there are per month, in this case we are going to get the average value of `ARRIVAL_DELAY` per month. The first column is the key (Number of month) and the second value is the average arrival time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 5.65),\n",
       " (2, 7.72),\n",
       " (3, 4.89),\n",
       " (4, 3.14),\n",
       " (5, 4.64),\n",
       " (6, 9.53),\n",
       " (7, 6.7),\n",
       " (8, 4.65),\n",
       " (9, -0.84),\n",
       " (10, -0.54),\n",
       " (11, 0.82),\n",
       " (12, 6.04)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.map(lambda x: (x.MONTH, x.ARRIVAL_DELAY)).groupByKey().mapValues(list)\\\n",
    "        .mapValues(lambda x: round(sum(x)/len(x), 2)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Working with DataFrame <a class=\"anchor\" id=\"2\"></a>\n",
    "## 2.1. Data Preparation and Loading <a class=\"anchor\" id=\"2.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Define dataframes and loading scheme<a class=\"anchor\" id=\"2.1.1\"></a>\n",
    "[Back to top](#table)\n",
    "\n",
    "To upload the data as dataframe, we will use a function that allow the user to input the path of the CSV files, and convert them into a data frame with header and a schema infered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_DF(path): # Path of the file, list with the name of the columns that are int and float\n",
    "     # Read the CSV file and convert it into a data frame with header, besides it has to infer the Schema\n",
    "    df = spark.read.format(\"csv\").options(header='true',inferSchema='true').load(path)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Display the schema of the final two dataframes<a class=\"anchor\" id=\"2.1.2\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flights\n",
    "Now that the function is created we proceed to import the data, the specifications of the assignment says that we have to display the dataframe, but as this one has 31 columns it would be messy. Therefore, we will just display the schema of the data frame. The full dataframe would be display by using `df.show()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- MONTH: integer (nullable = true)\n",
      " |-- DAY: integer (nullable = true)\n",
      " |-- DAY_OF_WEEK: integer (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      " |-- FLIGHT_NUMBER: integer (nullable = true)\n",
      " |-- TAIL_NUMBER: string (nullable = true)\n",
      " |-- ORIGIN_AIRPORT: string (nullable = true)\n",
      " |-- DESTINATION_AIRPORT: string (nullable = true)\n",
      " |-- SCHEDULED_DEPARTURE: integer (nullable = true)\n",
      " |-- DEPARTURE_TIME: integer (nullable = true)\n",
      " |-- DEPARTURE_DELAY: integer (nullable = true)\n",
      " |-- TAXI_OUT: integer (nullable = true)\n",
      " |-- WHEELS_OFF: integer (nullable = true)\n",
      " |-- SCHEDULED_TIME: integer (nullable = true)\n",
      " |-- ELAPSED_TIME: integer (nullable = true)\n",
      " |-- AIR_TIME: integer (nullable = true)\n",
      " |-- DISTANCE: integer (nullable = true)\n",
      " |-- WHEELS_ON: integer (nullable = true)\n",
      " |-- TAXI_IN: integer (nullable = true)\n",
      " |-- SCHEDULED_ARRIVAL: integer (nullable = true)\n",
      " |-- ARRIVAL_TIME: integer (nullable = true)\n",
      " |-- ARRIVAL_DELAY: integer (nullable = true)\n",
      " |-- DIVERTED: integer (nullable = true)\n",
      " |-- CANCELLED: integer (nullable = true)\n",
      " |-- CANCELLATION_REASON: string (nullable = true)\n",
      " |-- AIR_SYSTEM_DELAY: integer (nullable = true)\n",
      " |-- SECURITY_DELAY: integer (nullable = true)\n",
      " |-- AIRLINE_DELAY: integer (nullable = true)\n",
      " |-- LATE_AIRCRAFT_DELAY: integer (nullable = true)\n",
      " |-- WEATHER_DELAY: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsDf = csv_to_DF('./flight-delays/flight*.csv')\n",
    "flightsDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airports\n",
    "We do the same with the airports dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- IATA_CODE: string (nullable = true)\n",
      " |-- AIRPORT: string (nullable = true)\n",
      " |-- CITY: string (nullable = true)\n",
      " |-- STATE: string (nullable = true)\n",
      " |-- COUNTRY: string (nullable = true)\n",
      " |-- LATITUDE: double (nullable = true)\n",
      " |-- LONGITUDE: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airportsDf = csv_to_DF('./flight-delays/airports.csv')\n",
    "airportsDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that in both cases, the code transformed the that itself. It infered the schema itself, so in this case we did not need to transform each column manually. This was achieved using \n",
    "```python\n",
    "inferSchema = 'true'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Query Analysis <a class=\"anchor\" id=\"2.2\"></a>\n",
    "### 2.2.1 January flight events with ANC airport <a class=\"anchor\" id=\"2.2.1\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyspark\n",
    "To do this query using Pyspark we need to import the function `col` to use filters over the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can proceed with the query, filtering the `YEAR` before we remove it from the dataset. Then, getting the columns we are interested in, following by filtering `MONTH` and `ORIGIN_AIRPORT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "janFlightEventsAncDf = flightsDf.filter(col('YEAR') == 2015)\\\n",
    "                                .select('MONTH', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'DISTANCE', 'ARRIVAL_DELAY')\\\n",
    "                                .filter(col('MONTH') == 1)\\\n",
    "                                .filter(col('ORIGIN_AIRPORT') == 'ANC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the query that we wanted, we proceed to display the table and the number of rows we extracted from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The query has 115 rows\n",
      "+-----+--------------+-------------------+--------+-------------+\n",
      "|MONTH|ORIGIN_AIRPORT|DESTINATION_AIRPORT|DISTANCE|ARRIVAL_DELAY|\n",
      "+-----+--------------+-------------------+--------+-------------+\n",
      "|    1|           ANC|                SEA|    1448|          -13|\n",
      "|    1|           ANC|                SEA|    1448|           -4|\n",
      "|    1|           ANC|                JNU|     571|           17|\n",
      "|    1|           ANC|                CDV|     160|           20|\n",
      "|    1|           ANC|                BET|     399|          -20|\n",
      "|    1|           ANC|                SEA|    1448|          -15|\n",
      "|    1|           ANC|                SEA|    1448|          -11|\n",
      "|    1|           ANC|                ADQ|     253|          -16|\n",
      "|    1|           ANC|                SEA|    1448|           17|\n",
      "|    1|           ANC|                BET|     399|           -9|\n",
      "|    1|           ANC|                SEA|    1448|           15|\n",
      "|    1|           ANC|                FAI|     261|           -6|\n",
      "|    1|           ANC|                JNU|     571|            2|\n",
      "|    1|           ANC|                JNU|     571|           -3|\n",
      "|    1|           ANC|                PDX|    1542|          -21|\n",
      "|    1|           ANC|                SEA|    1448|           -5|\n",
      "|    1|           ANC|                SEA|    1448|          -15|\n",
      "|    1|           ANC|                PDX|    1542|          -13|\n",
      "|    1|           ANC|                SFO|    2018|           20|\n",
      "|    1|           ANC|                FAI|     261|           56|\n",
      "+-----+--------------+-------------------+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('The query has', janFlightEventsAncDf.count(), 'rows')\n",
    "janFlightEventsAncDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL\n",
    "If we perform the query using SQL we should get the same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The query has 115  rows\n",
      "+-----+--------------+-------------------+--------+-------------+\n",
      "|MONTH|ORIGIN_AIRPORT|DESTINATION_AIRPORT|DISTANCE|ARRIVAL_DELAY|\n",
      "+-----+--------------+-------------------+--------+-------------+\n",
      "|    1|           ANC|                SEA|    1448|          -13|\n",
      "|    1|           ANC|                SEA|    1448|           -4|\n",
      "|    1|           ANC|                JNU|     571|           17|\n",
      "|    1|           ANC|                CDV|     160|           20|\n",
      "|    1|           ANC|                BET|     399|          -20|\n",
      "|    1|           ANC|                SEA|    1448|          -15|\n",
      "|    1|           ANC|                SEA|    1448|          -11|\n",
      "|    1|           ANC|                ADQ|     253|          -16|\n",
      "|    1|           ANC|                SEA|    1448|           17|\n",
      "|    1|           ANC|                BET|     399|           -9|\n",
      "|    1|           ANC|                SEA|    1448|           15|\n",
      "|    1|           ANC|                FAI|     261|           -6|\n",
      "|    1|           ANC|                JNU|     571|            2|\n",
      "|    1|           ANC|                JNU|     571|           -3|\n",
      "|    1|           ANC|                PDX|    1542|          -21|\n",
      "|    1|           ANC|                SEA|    1448|           -5|\n",
      "|    1|           ANC|                SEA|    1448|          -15|\n",
      "|    1|           ANC|                PDX|    1542|          -13|\n",
      "|    1|           ANC|                SFO|    2018|           20|\n",
      "|    1|           ANC|                FAI|     261|           56|\n",
      "+-----+--------------+-------------------+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# register the original DataFrame as a temp view so that we can query it using SQL\n",
    "flightsDf.createOrReplaceTempView(\"flightsSQL\")\n",
    "janFlightEventsAncDf_SQL = spark.sql('''\n",
    "  SELECT MONTH, ORIGIN_AIRPORT, DESTINATION_AIRPORT, DISTANCE, ARRIVAL_DELAY FROM flightsSQL\n",
    "  WHERE YEAR = 2015 \n",
    "  AND MONTH = 1 \n",
    "  AND ORIGIN_AIRPORT = 'ANC'\n",
    "''')\n",
    "print('The query has', janFlightEventsAncDf_SQL.count(), ' rows')\n",
    "janFlightEventsAncDf_SQL.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By double checking with `Pyspark` and `SQL` we discovered, that there are **115** different flights in January 2015 which their origin airport is ANC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Average Arrival Delay From Origin to Destination <a class=\"anchor\" id=\"2.2.2\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import two different functions, `avg` to calculate the average value when aggregating, and `round` to get float numbers with a fixed number of decimals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "from pyspark.sql.functions import round"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did before, we are going to get this query using Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "janFlightEventsAncAvgDf = janFlightEventsAncDf.groupBy(\"ORIGIN_AIRPORT\", \"DESTINATION_AIRPORT\")\\\n",
    "                                                .agg(round(avg(\"ARRIVAL_DELAY\"), 2).alias(\"AVG_ARRIVAL_DELAY\"))\\\n",
    "                                                .orderBy('AVG_ARRIVAL_DELAY', ascending=False)\n",
    "                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The query has 18 rows\n",
      "+--------------+-------------------+-----------------+\n",
      "|ORIGIN_AIRPORT|DESTINATION_AIRPORT|AVG_ARRIVAL_DELAY|\n",
      "+--------------+-------------------+-----------------+\n",
      "|           ANC|                FAI|             25.0|\n",
      "|           ANC|                SFO|             20.0|\n",
      "|           ANC|                SCC|            16.67|\n",
      "|           ANC|                LAS|              9.0|\n",
      "|           ANC|                JNU|              5.0|\n",
      "|           ANC|                PDX|              3.5|\n",
      "|           ANC|                DEN|             3.33|\n",
      "|           ANC|                PHX|              2.0|\n",
      "|           ANC|                OTZ|             1.25|\n",
      "|           ANC|                CDV|              1.0|\n",
      "|           ANC|                ADQ|            -2.67|\n",
      "|           ANC|                OME|             -3.0|\n",
      "|           ANC|                BRW|            -4.33|\n",
      "|           ANC|                SEA|            -6.49|\n",
      "|           ANC|                BET|            -9.09|\n",
      "|           ANC|                MSP|           -19.25|\n",
      "|           ANC|                HNL|            -20.0|\n",
      "|           ANC|                ADK|            -27.0|\n",
      "+--------------+-------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('The query has', janFlightEventsAncAvgDf.count(), 'rows')\n",
    "janFlightEventsAncAvgDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL\n",
    "And we should get the same result if we use SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The query has 18 rows\n",
      "+--------------+-------------------+-----------------+\n",
      "|ORIGIN_AIRPORT|DESTINATION_AIRPORT|AVG_ARRIVAL_DELAY|\n",
      "+--------------+-------------------+-----------------+\n",
      "|           ANC|                FAI|             25.0|\n",
      "|           ANC|                SFO|             20.0|\n",
      "|           ANC|                SCC|            16.67|\n",
      "|           ANC|                LAS|              9.0|\n",
      "|           ANC|                JNU|              5.0|\n",
      "|           ANC|                PDX|              3.5|\n",
      "|           ANC|                DEN|             3.33|\n",
      "|           ANC|                PHX|              2.0|\n",
      "|           ANC|                OTZ|             1.25|\n",
      "|           ANC|                CDV|              1.0|\n",
      "|           ANC|                ADQ|            -2.67|\n",
      "|           ANC|                OME|             -3.0|\n",
      "|           ANC|                BRW|            -4.33|\n",
      "|           ANC|                SEA|            -6.49|\n",
      "|           ANC|                BET|            -9.09|\n",
      "|           ANC|                MSP|           -19.25|\n",
      "|           ANC|                HNL|            -20.0|\n",
      "|           ANC|                ADK|            -27.0|\n",
      "+--------------+-------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# register the original DataFrame as a temp view so that we can query it using SQL\n",
    "janFlightEventsAncDf.createOrReplaceTempView(\"janFlightEventsAncDfSQL\")\n",
    "janFlightEventsAncAvgDf_SQL = spark.sql('''\n",
    "  SELECT ORIGIN_AIRPORT, DESTINATION_AIRPORT, ROUND(AVG(ARRIVAL_DELAY), 2) AS AVG_ARRIVAL_DELAY \n",
    "  FROM janFlightEventsAncDfSQL\n",
    "  GROUP BY ORIGIN_AIRPORT, DESTINATION_AIRPORT\n",
    "  ORDER BY AVG_ARRIVAL_DELAY DESC\n",
    "''')\n",
    "print('The query has', janFlightEventsAncAvgDf_SQL.count(), 'rows')\n",
    "janFlightEventsAncAvgDf_SQL.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, by double checking with `Pyspark` and `SQL` we discovered, that there are **18** different combination of flights between ANC and other airports, besides we can see that the average delay in each route is different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Join Query with Airports DataFrame <a class=\"anchor\" id=\"2.2.3\"></a>\n",
    "[Back to top](#table)                               \n",
    "\n",
    "To perform the inner join we will use the `IATA_CODE` column of the airportsDf as the key column. This key could be join with `ORIGIN_AIRPORT` or `DESTINATION_AIRPORT`. That is why we are going to join them using both, so we can provide more information about the routes. In this task we will perform 2 different types of join that we could do: Sort-Merge Join and Broadcast Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort-Merge Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using ORIGIN_AIRPORT as key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The join has 10 columns\n",
      "The join has 18 rows\n",
      "+--------------+-------------------+-----------------+---------+--------------------+---------+-----+-------+--------+----------+\n",
      "|ORIGIN_AIRPORT|DESTINATION_AIRPORT|AVG_ARRIVAL_DELAY|IATA_CODE|             AIRPORT|     CITY|STATE|COUNTRY|LATITUDE| LONGITUDE|\n",
      "+--------------+-------------------+-----------------+---------+--------------------+---------+-----+-------+--------+----------+\n",
      "|           ANC|                BRW|            -4.33|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                ADK|            -27.0|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                OME|             -3.0|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                JNU|              5.0|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                LAS|              9.0|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                SCC|            16.67|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                CDV|              1.0|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                DEN|             3.33|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                OTZ|             1.25|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                SFO|             20.0|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                FAI|             25.0|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                ADQ|            -2.67|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                PDX|              3.5|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                PHX|              2.0|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                HNL|            -20.0|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                SEA|            -6.49|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                MSP|           -19.25|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "|           ANC|                BET|            -9.09|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|\n",
      "+--------------+-------------------+-----------------+---------+--------------------+---------+-----+-------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinedSqlDf = janFlightEventsAncAvgDf.join(airportsDf,\\\n",
    "                                           janFlightEventsAncAvgDf.ORIGIN_AIRPORT==airportsDf.IATA_CODE,how='inner')\n",
    "print('The join has', len(joinedSqlDf.columns), 'columns')\n",
    "print('The join has', joinedSqlDf.count(), 'rows')\n",
    "joinedSqlDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using DESTINATION_AIRPORT as key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The join has 10 columns\n",
      "The join has 18 rows\n",
      "+--------------+-------------------+-----------------+---------+--------------------+-------------+-----+-------+--------+----------+\n",
      "|ORIGIN_AIRPORT|DESTINATION_AIRPORT|AVG_ARRIVAL_DELAY|IATA_CODE|             AIRPORT|         CITY|STATE|COUNTRY|LATITUDE| LONGITUDE|\n",
      "+--------------+-------------------+-----------------+---------+--------------------+-------------+-----+-------+--------+----------+\n",
      "|           ANC|                BRW|            -4.33|      BRW|Wiley Post-Will R...|       Barrow|   AK|    USA|71.28545|  -156.766|\n",
      "|           ANC|                ADK|            -27.0|      ADK|        Adak Airport|         Adak|   AK|    USA|51.87796|-176.64603|\n",
      "|           ANC|                OME|             -3.0|      OME|        Nome Airport|         Nome|   AK|    USA| 64.5122|-165.44525|\n",
      "|           ANC|                JNU|              5.0|      JNU|Juneau Internatio...|       Juneau|   AK|    USA|58.35496|-134.57628|\n",
      "|           ANC|                LAS|              9.0|      LAS|McCarran Internat...|    Las Vegas|   NV|    USA|36.08036|-115.15233|\n",
      "|           ANC|                SCC|            16.67|      SCC|Deadhorse Airport...|    Deadhorse|   AK|    USA|70.19476|-148.46516|\n",
      "|           ANC|                CDV|              1.0|      CDV|Merle K. (Mudhole...|      Cordova|   AK|    USA|60.49183|-145.47765|\n",
      "|           ANC|                DEN|             3.33|      DEN|Denver Internatio...|       Denver|   CO|    USA|39.85841|  -104.667|\n",
      "|           ANC|                OTZ|             1.25|      OTZ|Ralph Wien Memori...|     Kotzebue|   AK|    USA|66.88468|-162.59855|\n",
      "|           ANC|                SFO|             20.0|      SFO|San Francisco Int...|San Francisco|   CA|    USA|  37.619|-122.37484|\n",
      "|           ANC|                FAI|             25.0|      FAI|Fairbanks Interna...|    Fairbanks|   AK|    USA|64.81368|-147.85967|\n",
      "|           ANC|                ADQ|            -2.67|      ADQ|      Kodiak Airport|       Kodiak|   AK|    USA|57.74997|-152.49386|\n",
      "|           ANC|                PDX|              3.5|      PDX|Portland Internat...|     Portland|   OR|    USA|45.58872| -122.5975|\n",
      "|           ANC|                PHX|              2.0|      PHX|Phoenix Sky Harbo...|      Phoenix|   AZ|    USA|33.43417|-112.00806|\n",
      "|           ANC|                HNL|            -20.0|      HNL|Honolulu Internat...|     Honolulu|   HI|    USA|21.31869|-157.92241|\n",
      "|           ANC|                SEA|            -6.49|      SEA|Seattle-Tacoma In...|      Seattle|   WA|    USA|47.44898|-122.30931|\n",
      "|           ANC|                MSP|           -19.25|      MSP|Minneapolis-Saint...|  Minneapolis|   MN|    USA|44.88055| -93.21692|\n",
      "|           ANC|                BET|            -9.09|      BET|      Bethel Airport|       Bethel|   AK|    USA|60.77978|  -161.838|\n",
      "+--------------+-------------------+-----------------+---------+--------------------+-------------+-----+-------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinedSqlDf = janFlightEventsAncAvgDf.join(airportsDf,\\\n",
    "                                           janFlightEventsAncAvgDf.DESTINATION_AIRPORT==airportsDf.IATA_CODE,how='inner')\n",
    "print('The join has', len(joinedSqlDf.columns), 'columns')\n",
    "print('The join has', joinedSqlDf.count(), 'rows')\n",
    "joinedSqlDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcast Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using ORIGIN_AIRPORT as key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The join has 10 columns\n",
      "The join has 18 rows\n",
      "+---------+--------------------+---------+-----+-------+--------+----------+--------------+-------------------+-----------------+\n",
      "|IATA_CODE|             AIRPORT|     CITY|STATE|COUNTRY|LATITUDE| LONGITUDE|ORIGIN_AIRPORT|DESTINATION_AIRPORT|AVG_ARRIVAL_DELAY|\n",
      "+---------+--------------------+---------+-----+-------+--------+----------+--------------+-------------------+-----------------+\n",
      "|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|           ANC|                BET|            -9.09|\n",
      "|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|           ANC|                MSP|           -19.25|\n",
      "|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|           ANC|                SEA|            -6.49|\n",
      "|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|           ANC|                HNL|            -20.0|\n",
      "|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|           ANC|                PHX|              2.0|\n",
      "|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|           ANC|                PDX|              3.5|\n",
      "|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|           ANC|                ADQ|            -2.67|\n",
      "|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|           ANC|                FAI|             25.0|\n",
      "|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|           ANC|                SFO|             20.0|\n",
      "|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|           ANC|                OTZ|             1.25|\n",
      "|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|           ANC|                DEN|             3.33|\n",
      "|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|           ANC|                CDV|              1.0|\n",
      "|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|           ANC|                SCC|            16.67|\n",
      "|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|           ANC|                LAS|              9.0|\n",
      "|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|           ANC|                JNU|              5.0|\n",
      "|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|           ANC|                OME|             -3.0|\n",
      "|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|           ANC|                ADK|            -27.0|\n",
      "|      ANC|Ted Stevens Ancho...|Anchorage|   AK|    USA|61.17432|-149.99619|           ANC|                BRW|            -4.33|\n",
      "+---------+--------------------+---------+-----+-------+--------+----------+--------------+-------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "joinedSqlDf = airportsDf.join(broadcast(janFlightEventsAncAvgDf),\\\n",
    "                              janFlightEventsAncAvgDf.ORIGIN_AIRPORT==airportsDf.IATA_CODE,how='inner')\n",
    "print('The join has', len(joinedSqlDf.columns), 'columns')\n",
    "print('The join has', joinedSqlDf.count(), 'rows')\n",
    "joinedSqlDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using DESTINATION_AIRPORT as key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The join has 10 columns\n",
      "The join has 18 rows\n",
      "+---------+--------------------+-------------+-----+-------+--------+----------+--------------+-------------------+-----------------+\n",
      "|IATA_CODE|             AIRPORT|         CITY|STATE|COUNTRY|LATITUDE| LONGITUDE|ORIGIN_AIRPORT|DESTINATION_AIRPORT|AVG_ARRIVAL_DELAY|\n",
      "+---------+--------------------+-------------+-----+-------+--------+----------+--------------+-------------------+-----------------+\n",
      "|      ADK|        Adak Airport|         Adak|   AK|    USA|51.87796|-176.64603|           ANC|                ADK|            -27.0|\n",
      "|      ADQ|      Kodiak Airport|       Kodiak|   AK|    USA|57.74997|-152.49386|           ANC|                ADQ|            -2.67|\n",
      "|      BET|      Bethel Airport|       Bethel|   AK|    USA|60.77978|  -161.838|           ANC|                BET|            -9.09|\n",
      "|      BRW|Wiley Post-Will R...|       Barrow|   AK|    USA|71.28545|  -156.766|           ANC|                BRW|            -4.33|\n",
      "|      CDV|Merle K. (Mudhole...|      Cordova|   AK|    USA|60.49183|-145.47765|           ANC|                CDV|              1.0|\n",
      "|      DEN|Denver Internatio...|       Denver|   CO|    USA|39.85841|  -104.667|           ANC|                DEN|             3.33|\n",
      "|      FAI|Fairbanks Interna...|    Fairbanks|   AK|    USA|64.81368|-147.85967|           ANC|                FAI|             25.0|\n",
      "|      HNL|Honolulu Internat...|     Honolulu|   HI|    USA|21.31869|-157.92241|           ANC|                HNL|            -20.0|\n",
      "|      JNU|Juneau Internatio...|       Juneau|   AK|    USA|58.35496|-134.57628|           ANC|                JNU|              5.0|\n",
      "|      LAS|McCarran Internat...|    Las Vegas|   NV|    USA|36.08036|-115.15233|           ANC|                LAS|              9.0|\n",
      "|      MSP|Minneapolis-Saint...|  Minneapolis|   MN|    USA|44.88055| -93.21692|           ANC|                MSP|           -19.25|\n",
      "|      OME|        Nome Airport|         Nome|   AK|    USA| 64.5122|-165.44525|           ANC|                OME|             -3.0|\n",
      "|      OTZ|Ralph Wien Memori...|     Kotzebue|   AK|    USA|66.88468|-162.59855|           ANC|                OTZ|             1.25|\n",
      "|      PDX|Portland Internat...|     Portland|   OR|    USA|45.58872| -122.5975|           ANC|                PDX|              3.5|\n",
      "|      PHX|Phoenix Sky Harbo...|      Phoenix|   AZ|    USA|33.43417|-112.00806|           ANC|                PHX|              2.0|\n",
      "|      SCC|Deadhorse Airport...|    Deadhorse|   AK|    USA|70.19476|-148.46516|           ANC|                SCC|            16.67|\n",
      "|      SEA|Seattle-Tacoma In...|      Seattle|   WA|    USA|47.44898|-122.30931|           ANC|                SEA|            -6.49|\n",
      "|      SFO|San Francisco Int...|San Francisco|   CA|    USA|  37.619|-122.37484|           ANC|                SFO|             20.0|\n",
      "+---------+--------------------+-------------+-----+-------+--------+----------+--------------+-------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinedSqlDf = airportsDf.join(broadcast(janFlightEventsAncAvgDf),\\\n",
    "                              janFlightEventsAncAvgDf.DESTINATION_AIRPORT==airportsDf.IATA_CODE,how='inner')\n",
    "print('The join has', len(joinedSqlDf.columns), 'columns')\n",
    "print('The join has', joinedSqlDf.count(), 'rows')\n",
    "joinedSqlDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the joins above we can see that no matter with join strategy we use, `sort-merge` or `broadcast` join, we will get the same amount of rows and columns, the only difference would come up if we change the key. When selecting `ORIGIN_AIRPORT` as the key of joining we get 18 rows with the same values, but if we use `DESTINATION_AIRPORT` as the key we will get different information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Analysis <a class=\"anchor\" id=\"2.3\"></a>\n",
    "### 2.3.1 Relationship between day of week with mean arrival delay, total time delay, and count flights <a class=\"anchor\" id=\"2.3.1\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The query has 4 columns\n",
      "The query has 7 rows\n",
      "+-----------+----------------+--------------+------------+\n",
      "|DAY_OF_WEEK|MeanArrivalDelay|TotalTimeDelay|NumOfFlights|\n",
      "+-----------+----------------+--------------+------------+\n",
      "|          4|            5.68|        490186|       86227|\n",
      "|          5|            4.72|        401638|       85181|\n",
      "|          3|            3.97|        335150|       84324|\n",
      "|          1|            5.88|        494478|       84052|\n",
      "|          2|            4.39|        363262|       82719|\n",
      "|          7|             4.3|        343498|       79898|\n",
      "|          6|            1.81|        125750|       69328|\n",
      "+-----------+----------------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum as _sum\n",
    "import pyspark.sql.functions as func\n",
    "\n",
    "delayDayOfWeek = flightsDf.groupBy(\"DAY_OF_WEEK\")\\\n",
    "                            .agg(round(avg(\"ARRIVAL_DELAY\"), 2).alias(\"MeanArrivalDelay\"),\\\n",
    "                                 round(_sum(\"ARRIVAL_DELAY\"), 2).alias(\"TotalTimeDelay\"),\\\n",
    "                                 func.count('ARRIVAL_DELAY').alias(\"NumOfFlights\"))\\\n",
    "                            .orderBy(\"NumOfFlights\", ascending=False)\n",
    "\n",
    "print('The query has', len(delayDayOfWeek.columns), 'columns')\n",
    "print('The query has', delayDayOfWeek.count(), 'rows')\n",
    "delayDayOfWeek.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The query has 4 columns\n",
      "The query has 7 rows\n",
      "+-----------+----------------+--------------+------------+\n",
      "|DAY_OF_WEEK|MeanArrivalDelay|TotalTimeDelay|NumOfFlights|\n",
      "+-----------+----------------+--------------+------------+\n",
      "|          4|            5.68|        490186|       86227|\n",
      "|          5|            4.72|        401638|       85181|\n",
      "|          3|            3.97|        335150|       84324|\n",
      "|          1|            5.88|        494478|       84052|\n",
      "|          2|            4.39|        363262|       82719|\n",
      "|          7|             4.3|        343498|       79898|\n",
      "|          6|            1.81|        125750|       69328|\n",
      "+-----------+----------------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# register the original DataFrame as a temp view so that we can query it using SQL\n",
    "flightsDf.createOrReplaceTempView(\"flightsDfSQL\")\n",
    "delayDayOfWeek_SQL = spark.sql('''\n",
    "  SELECT DAY_OF_WEEK, ROUND(AVG(ARRIVAL_DELAY), 2) AS MeanArrivalDelay, ROUND(SUM(ARRIVAL_DELAY), 2) AS TotalTimeDelay,\n",
    "  COUNT(ARRIVAL_DELAY) AS NumOfFlights FROM flightsDfSQL\n",
    "  GROUP BY DAY_OF_WEEK\n",
    "  ORDER BY NumOfFlights DESC\n",
    "''')\n",
    "\n",
    "print('The query has', len(delayDayOfWeek_SQL.columns), 'columns')\n",
    "print('The query has', delayDayOfWeek_SQL.count(), 'rows')\n",
    "delayDayOfWeek_SQL.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the query above we can identify many different things:\n",
    "1. The day of the week that with the most flights is Thrusday with 86.227 during the whole year\n",
    "2. The day of the week that with the least flights is Saturday with 69.328 during the whole year\n",
    "\n",
    "Following these two pieces of information we can also observe that Thrusday is the second day of the week with more mean arrival delay, and that Saturday is the day with least delay. Besides, all days have a mean of arrival delay pretty similar, except Saturday, as well as the number of flight. This information can lead us think that exists a pretty close relationship between the number of flights per day and the mean arrival delay. \n",
    "\n",
    "**The more flights in a day, more arrival delay**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Display mean arrival delay each month <a class=\"anchor\" id=\"2.3.2\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The query has 4 columns\n",
      "The query has 12 rows\n",
      "+-----+----------------+--------------+------------+\n",
      "|MONTH|MeanArrivalDelay|TotalTimeDelay|NumOfFlights|\n",
      "+-----+----------------+--------------+------------+\n",
      "|    9|           -0.85|        -39484|       46459|\n",
      "|   10|           -0.54|        -26209|       48357|\n",
      "|   11|            0.83|         38412|       46203|\n",
      "|    4|            3.17|        153044|       48221|\n",
      "|    8|            4.71|        235063|       49866|\n",
      "|    5|            4.71|        230785|       48977|\n",
      "|    3|            5.01|        248454|       49580|\n",
      "|    1|             5.8|        266420|       45900|\n",
      "|   12|            6.16|        288883|       46909|\n",
      "|    7|            6.79|        348907|       51415|\n",
      "|    2|            8.12|        330513|       40684|\n",
      "|    6|            9.75|        479174|       49158|\n",
      "+-----+----------------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delayMonth = flightsDf.groupBy(\"MONTH\")\\\n",
    "                            .agg(round(avg(\"ARRIVAL_DELAY\"), 2).alias(\"MeanArrivalDelay\"),\\\n",
    "                                 round(_sum(\"ARRIVAL_DELAY\"), 2).alias(\"TotalTimeDelay\"),\\\n",
    "                                 func.count('ARRIVAL_DELAY').alias(\"NumOfFlights\"))\\\n",
    "                            .orderBy(\"MeanArrivalDelay\", ascending=True)\n",
    "\n",
    "print('The query has', len(delayMonth.columns), 'columns')\n",
    "print('The query has', delayMonth.count(), 'rows')\n",
    "delayMonth.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The query has 4 columns\n",
      "The query has 12 rows\n",
      "+-----+----------------+--------------+------------+\n",
      "|MONTH|MeanArrivalDelay|TotalTimeDelay|NumOfFlights|\n",
      "+-----+----------------+--------------+------------+\n",
      "|    9|           -0.85|        -39484|       46459|\n",
      "|   10|           -0.54|        -26209|       48357|\n",
      "|   11|            0.83|         38412|       46203|\n",
      "|    4|            3.17|        153044|       48221|\n",
      "|    5|            4.71|        230785|       48977|\n",
      "|    8|            4.71|        235063|       49866|\n",
      "|    3|            5.01|        248454|       49580|\n",
      "|    1|             5.8|        266420|       45900|\n",
      "|   12|            6.16|        288883|       46909|\n",
      "|    7|            6.79|        348907|       51415|\n",
      "|    2|            8.12|        330513|       40684|\n",
      "|    6|            9.75|        479174|       49158|\n",
      "+-----+----------------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# register the original DataFrame as a temp view so that we can query it using SQL\n",
    "flightsDf.createOrReplaceTempView(\"flightsDfSQL\")\n",
    "delayMonth_SQL = spark.sql('''\n",
    "  SELECT MONTH, ROUND(AVG(ARRIVAL_DELAY), 2) AS MeanArrivalDelay, ROUND(SUM(ARRIVAL_DELAY), 2) AS TotalTimeDelay,\n",
    "  COUNT(ARRIVAL_DELAY) AS NumOfFlights FROM flightsDfSQL\n",
    "  GROUP BY MONTH\n",
    "  ORDER BY MeanArrivalDelay ASC\n",
    "''')\n",
    "\n",
    "print('The query has', len(delayMonth_SQL.columns), 'columns')\n",
    "print('The query has', delayMonth_SQL.count(), 'rows')\n",
    "delayMonth_SQL.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the query above we can identify many different things:\n",
    "1. The month of the year that with the least delay is September with -0.85 of mean arrival delay\n",
    "2. The month of the year that with the most flights is Saturday with 9.75 of mean arrival delay\n",
    "\n",
    "In this query we can not find a clear pattern between the number of flights and the mean arrival delay, as we did with the days of the week, which means that there are other factors different than the number of flights that can affect the mean arrival time. But unfortunately we do not have the information to find out that. \n",
    "\n",
    "Making some hypothesis, we could guess that the holiday season such as June, July, and December are the months that have more delay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Relationship between mean departure delay and mean arrival delay <a class=\"anchor\" id=\"2.3.3\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The query has 3 columns\n",
      "The query has 12 rows\n",
      "+-----+----------------+-------------+\n",
      "|MONTH|MeanArrivalDelay|MeanDeptDelay|\n",
      "+-----+----------------+-------------+\n",
      "|    6|            9.75|        13.97|\n",
      "|   12|            6.16|        11.82|\n",
      "|    7|            6.79|        11.71|\n",
      "|    2|            8.12|        11.62|\n",
      "|    8|            4.71|        10.09|\n",
      "|    1|             5.8|         9.75|\n",
      "|    3|            5.01|         9.72|\n",
      "|    5|            4.71|         9.55|\n",
      "|    4|            3.17|         7.74|\n",
      "|   11|            0.83|         6.63|\n",
      "|   10|           -0.54|         5.24|\n",
      "|    9|           -0.85|         4.73|\n",
      "+-----+----------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DepArrdelayMonth = flightsDf.groupBy(\"MONTH\")\\\n",
    "                            .agg(round(avg(\"ARRIVAL_DELAY\"), 2).alias(\"MeanArrivalDelay\"),\\\n",
    "                                 round(avg(\"DEPARTURE_DELAY\"), 2).alias(\"MeanDeptDelay\"))\\\n",
    "                            .orderBy(\"MeanDeptDelay\", ascending=False)\n",
    "\n",
    "print('The query has', len(DepArrdelayMonth.columns), 'columns')\n",
    "print('The query has', DepArrdelayMonth.count(), 'rows')\n",
    "DepArrdelayMonth.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The query has 3 columns\n",
      "The query has 12 rows\n",
      "+-----+----------------+-------------+\n",
      "|MONTH|MeanArrivalDelay|MeanDeptDelay|\n",
      "+-----+----------------+-------------+\n",
      "|    6|            9.75|        13.97|\n",
      "|    2|            8.12|        11.62|\n",
      "|    7|            6.79|        11.71|\n",
      "|   12|            6.16|        11.82|\n",
      "|    1|             5.8|         9.75|\n",
      "|    3|            5.01|         9.72|\n",
      "|    5|            4.71|         9.55|\n",
      "|    8|            4.71|        10.09|\n",
      "|    4|            3.17|         7.74|\n",
      "|   11|            0.83|         6.63|\n",
      "|   10|           -0.54|         5.24|\n",
      "|    9|           -0.85|         4.73|\n",
      "+-----+----------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# register the original DataFrame as a temp view so that we can query it using SQL\n",
    "flightsDf.createOrReplaceTempView(\"flightsDfSQL\")\n",
    "DepArrdelayMonth_SQL = spark.sql('''\n",
    "  SELECT MONTH, ROUND(AVG(ARRIVAL_DELAY), 2) AS MeanArrivalDelay, ROUND(AVG(DEPARTURE_DELAY), 2) AS MeanDeptDelay \n",
    "  FROM flightsDfSQL\n",
    "  GROUP BY MONTH\n",
    "  ORDER BY MeanArrivalDelay DESC\n",
    "''')\n",
    "\n",
    "print('The query has', len(DepArrdelayMonth_SQL.columns), 'columns')\n",
    "print('The query has', DepArrdelayMonth_SQL.count(), 'rows')\n",
    "DepArrdelayMonth_SQL.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comparing the mean arrival delay and the mean departure delay, we can see that they are closely related. The higher the departure delay, the higher the departure delay too. There is a positive correlation between these two variables, which means that if a plane has a departure delay is almost certain that it will have a departure delay. \n",
    "\n",
    "Even though they are closely related, the mean arrival delay is always smaller than the departure delay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 RDDs vs DataFrame vs Spark SQL <a class=\"anchor\" id=\"3\"></a>\n",
    "\n",
    "\n",
    "Implement the following queries using RDDs, DataFrames and SparkSQL separately. Log the time taken for each query in each approach using the “%%time” built-in magic command in Jupyter Notebook and discuss the performance difference of these 3 approaches.\n",
    "\n",
    "<strong>Find the MONTH and DAY_OF_WEEK, number of flights, and average delay where TAIL_NUMBER = ‘N407AS’. Note number of flights and average delay should be aggregated separately. The average delay should be grouped by both MONTH and DAYS_OF_WEEK.</strong>\n",
    "\n",
    "## 3.1 RDD Operation<a class=\"anchor\" id=\"3.1\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.8 ms ± 1.86 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "CPU times: user 1.11 s, sys: 73.3 ms, total: 1.18 s\n",
      "Wall time: 2.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%timeit\n",
    "\n",
    "rdd_numFlights = flights.filter(lambda x: x.TAIL_NUMBER == 'N407AS')\\\n",
    "                        .map(lambda x: ((x.MONTH, x.DAY_OF_WEEK), x.ARRIVAL_DELAY))\\\n",
    "                        .groupByKey().mapValues(list)\\\n",
    "                        .mapValues(lambda x: (len(x), sum(x)/len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((11, 7), (3, -4.0)),\n",
       " ((3, 3), (1, 3.0)),\n",
       " ((4, 6), (1, -20.0)),\n",
       " ((12, 2), (2, -11.5)),\n",
       " ((5, 5), (1, 6.0)),\n",
       " ((6, 3), (3, -10.666666666666666)),\n",
       " ((7, 4), (2, -4.0)),\n",
       " ((8, 1), (2, -13.0)),\n",
       " ((9, 2), (1, -10.0)),\n",
       " ((6, 2), (1, 35.0)),\n",
       " ((9, 3), (5, -14.6)),\n",
       " ((7, 5), (1, -4.0)),\n",
       " ((8, 7), (2, 60.5)),\n",
       " ((10, 1), (2, 15.5)),\n",
       " ((9, 4), (3, -10.666666666666666))]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_numFlights.take(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 DataFrame Operation<a class=\"anchor\" id=\"3.2\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.8 ms ± 2.84 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "CPU times: user 384 ms, sys: 73 ms, total: 457 ms\n",
      "Wall time: 2.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%timeit\n",
    "\n",
    "\n",
    "df_numFlights = flightsDf.filter(flightsDf.TAIL_NUMBER == 'N407AS')\\\n",
    "                            .groupBy(\"MONTH\", \"DAY_OF_WEEK\")\\\n",
    "                            .agg(func.count('ARRIVAL_DELAY').alias(\"NumOfFlights\"),\\\n",
    "                                 round(avg(\"ARRIVAL_DELAY\"), 2).alias(\"MeanArrivalDelay\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+------------+----------------+\n",
      "|MONTH|DAY_OF_WEEK|NumOfFlights|MeanArrivalDelay|\n",
      "+-----+-----------+------------+----------------+\n",
      "|    6|          1|           4|             7.0|\n",
      "|    3|          1|           1|            29.0|\n",
      "|    7|          4|           2|            -4.0|\n",
      "|    2|          2|           2|            -9.5|\n",
      "|    9|          4|           3|          -10.67|\n",
      "|   12|          7|           2|            -1.0|\n",
      "|    8|          3|           1|            -4.0|\n",
      "|    4|          7|           1|           -22.0|\n",
      "|    2|          3|           2|           -11.5|\n",
      "|    7|          1|           1|            -1.0|\n",
      "|   12|          2|           2|           -11.5|\n",
      "|    1|          2|           2|            17.5|\n",
      "|   11|          1|           1|            35.0|\n",
      "|    9|          1|           2|           -15.5|\n",
      "|    5|          7|           3|           -7.67|\n",
      "|    5|          6|           2|            -3.0|\n",
      "|   12|          1|           1|            -1.0|\n",
      "|   10|          5|           3|           -3.67|\n",
      "|    1|          1|           1|            -6.0|\n",
      "|    1|          3|           1|           -27.0|\n",
      "+-----+-----------+------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_numFlights.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Spark SQL OPERATION<a class=\"anchor\" id=\"3.3\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.3 ms ± 2.72 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "CPU times: user 52.1 ms, sys: 1.33 ms, total: 53.4 ms\n",
      "Wall time: 3.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%timeit\n",
    "\n",
    "# register the original DataFrame as a temp view so that we can query it using SQL\n",
    "flightsDf.createOrReplaceTempView(\"flightsDfSQL\")\n",
    "df_numFlights_SQL = spark.sql('''\n",
    "  SELECT MONTH, DAY_OF_WEEK, COUNT(ARRIVAL_DELAY) AS NumOfFlights, ROUND(AVG(ARRIVAL_DELAY), 2) AS MeanArrivalDelay, \n",
    "  ROUND(AVG(DEPARTURE_DELAY), 2) AS MeanDeptDelay\n",
    "  FROM flightsDfSQL\n",
    "  WHERE TAIL_NUMBER == 'N407AS'\n",
    "  GROUP BY MONTH, DAY_OF_WEEK\n",
    "  ORDER BY MeanDeptDelay DESC\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+------------+----------------+-------------+\n",
      "|MONTH|DAY_OF_WEEK|NumOfFlights|MeanArrivalDelay|MeanDeptDelay|\n",
      "+-----+-----------+------------+----------------+-------------+\n",
      "|    8|          7|           2|            60.5|         66.5|\n",
      "|   11|          1|           1|            35.0|         57.0|\n",
      "|    3|          1|           1|            29.0|         40.0|\n",
      "|    6|          2|           1|            35.0|         33.0|\n",
      "|    3|          3|           1|             3.0|         28.0|\n",
      "|    5|          5|           1|             6.0|         17.0|\n",
      "|    7|          7|           4|            19.0|         15.8|\n",
      "|    1|          2|           2|            17.5|         12.5|\n",
      "|   10|          1|           2|            15.5|         12.5|\n",
      "|    7|          5|           1|            -4.0|          9.0|\n",
      "|    1|          6|           3|            4.33|         8.67|\n",
      "|    9|          2|           1|           -10.0|          8.0|\n",
      "|    5|          2|           5|             0.8|          6.0|\n",
      "|    3|          5|           3|            6.67|         5.67|\n",
      "|    5|          7|           3|           -7.67|         4.67|\n",
      "|    6|          1|           4|             7.0|          4.5|\n",
      "|    1|          1|           1|            -6.0|          4.0|\n",
      "|    9|          5|           1|             5.0|          4.0|\n",
      "|   12|          4|           1|             6.0|          2.0|\n",
      "|    5|          1|           3|            4.67|          2.0|\n",
      "+-----+-----------+------------+----------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_numFlights_SQL.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Discussion<a class=\"anchor\" id=\"3.4\"></a>\n",
    "[Back to top](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing the same query using 3 different strategies `RDD`, `Pyspark Dataframe` and `SQL Dataframe` we find out pretty interesting things. The first one is the time that each query takes to perform, `RDD` is the quickest one, only taking **2.04s** to complete which means *~22.8ms* per loop. The sencond one is the `Pyspark Dataframe`, taking **2.51s** to complete or *~25.8ms* per loop. And the slowest one is the `SQL Dataframe` taking **3.59s** to complete or *~43.3ms* per loop. \n",
    "\n",
    "In the following table we can see the comparison between strategies:\n",
    "\n",
    "|Strategy|Total Time (s)|Time per Loop (ms)|% of delay vs. RDD|\n",
    "|:-:|:-:|:-:|:-:|\n",
    "|RDD|2.04|~22.8|-|\n",
    "|Pyspark|2.51|~25.8|23.3%|\n",
    "|SQL|3.59|~43.3|89.9%|\n",
    "\n",
    "From the table above we can understand some things, as it was already mentioned, the RDD strategy is the most efficient one if we only take in count the time of processing, but we also habe to be aware that when working with RDD the output is a list, and not a table, that is why the query does not have headers, which makes it not easy to read. Therefore, if we measure the efficiency by readability the `RDD` is not the best. In this case `Pyspark` would be the best, as the result is a dataset easy to read. While the only advantage that the `SQL dataframe` query could present is the readability of the code.\n",
    "\n",
    "In order to identify the best strategy to do the query, we must understand the purpuse of the query, and then we will understand which one is the best."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FIT5202 Assignment 1 SOLUTION.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
